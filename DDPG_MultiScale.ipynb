{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/tomoaki/work\")\n",
    "import my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_list = ['MMM', 'T', 'ABBV', 'ABT', 'ACN', 'AGN', 'ALL', 'GOOGL', \n",
    "              'GOOG', 'MO', 'AMZN', 'AXP', 'AIG', 'AMGN', 'AAPL', 'BAC', 'BIIB', \n",
    "              'BLK', 'BA', 'BMY', 'CVS', 'COF', 'CAT', 'CELG', 'CVX', 'CSCO', 'C', \n",
    "              'KO', 'CL', 'CMCSA', 'COP', 'COST', 'DHR', 'DOW', 'DUK', 'DD', 'EMC', \n",
    "              'EMR', 'EXC', 'XOM', 'FB', 'FDX', 'F', 'GD', 'GE', 'GM', 'GILD', 'GS', 'HAL', \n",
    "              'HD', 'HON', 'INTC', 'IBM', 'JPM', 'JNJ', 'KMI', 'LLY', 'LMT', 'LOW', 'MA', \n",
    "              'MCD', 'MDT', 'MRK', 'MET', 'MSFT', 'MDLZ', 'MON', 'MS', 'NKE', 'OXY', \n",
    "              'ORCL', 'PEP', 'PFE', 'PM', 'PG', 'QCOM', 'RTN', 'SLB', 'SPG', 'SO', 'SBUX', \n",
    "              'TGT', 'TXN', 'BK', 'PCLN', 'TWX', 'FOXA', 'FOX', 'USB', 'UNP', 'UPS', 'UTX', \n",
    "              'UNH', 'VZ', 'V', 'WMT', 'WBA', 'DIS', 'WFC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!!\n",
      "('fail_name_list: ', [])\n",
      "time for getting data: 295.864296913\n"
     ]
    }
   ],
   "source": [
    "print (\"Started!!\")\n",
    "\n",
    "st = time.time()\n",
    "start_date=\"2014-04-01\"\n",
    "end_date=\"2016-04-01\"\n",
    "input_data, input_list = my_utils.get_fixed_data(input_list, start_date=start_date, end_date=end_date) \n",
    "elapsed = time.time() - st\n",
    "print (\"time for getting data:\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(505, 99)\n"
     ]
    }
   ],
   "source": [
    "print (input_data.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2014-04-01', '2014-04-02', '2014-04-03', '2014-04-04',\n",
      "               '2014-04-07', '2014-04-08', '2014-04-09', '2014-04-10',\n",
      "               '2014-04-11', '2014-04-14',\n",
      "               ...\n",
      "               '2016-03-18', '2016-03-21', '2016-03-22', '2016-03-23',\n",
      "               '2016-03-24', '2016-03-28', '2016-03-29', '2016-03-30',\n",
      "               '2016-03-31', '2016-04-01'],\n",
      "              dtype='datetime64[ns]', length=505, freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(input_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG for trading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data, you are going to learn how to manage your portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# memory for replay\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Experiecne = namedtuple('Experience', 'state0, action, reward, state1')\n",
    "\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        # self.data = [None for _ in range(maxlen)]\n",
    "        self.data = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx< 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "    \n",
    "    def append(self, v):\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item\n",
    "            self.data[1:] = self.data[:-1]\n",
    "        else:\n",
    "            # This should never happen\n",
    "            raise RuntimeError()\n",
    "        self.data.append(v)\n",
    "        \n",
    "class SequentialMemory(object):\n",
    "    def __init__(self, limit=1000):\n",
    "        self.limit = limit\n",
    "        \n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "        \n",
    "    def sample(self, batch_size, window_length):\n",
    "        # draw random indexes such that is bigger than window_length to enough length data\n",
    "        batch_idx = np.random.random_integers(window_length, self.nb_entries - 1, size=batch_size)\n",
    "        assert len(batch_idx) == batch_size\n",
    "        \n",
    "        # create experiences\n",
    "        state0 = np.array([[self.observations[i] for i in range(idx - window_length,idx)] for idx in batch_idx])\n",
    "        action = np.array([self.actions[idx - 1] for idx in batch_idx])\n",
    "        reward = np.array([self.rewards[idx - 1] for idx in batch_idx])\n",
    "        state1 = np.array([[self.observations[i] for i in range(idx - window_length + 1,idx + 1)] for idx in batch_idx])\n",
    "        return Experiecne(state0, action, reward, state1)\n",
    "    \n",
    "    def append(self, observation, action, reward):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return  len(self.observations)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D \n",
    "from keras.layers.core import Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.engine.topology import Merge\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras import backend as K\n",
    "import time\n",
    "tf.python.control_flow_ops = tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"initialized approximate value function\n",
    "        config should have the following attributes\n",
    "        Args:\n",
    "            trade_stock_idx(int): trading stock index\n",
    "            gam (float): discount factor\n",
    "            n_history(int): nubmer of history that will be used as input\n",
    "        \"\"\"\n",
    "        self.activation = config.activation\n",
    "        self.gamma = config.gamma\n",
    "        self.history_length = config.history_length\n",
    "        self.n_stock = config.n_stock\n",
    "        self.n_feature = config.n_feature\n",
    "        self.n_smooth = config.n_smooth\n",
    "        self.n_down = config.n_down\n",
    "        self.k_w = config.k_w\n",
    "        self.n_hidden = config.n_hidden\n",
    "        self.n_batch = config.n_batch\n",
    "        self.n_epochs = config.n_epochs\n",
    "        self.update_rate = config.update_rate\n",
    "        self.lr = config.learning_rate\n",
    "        # the actual dimention of input\n",
    "        self.n_input = (1 + self.n_smooth + self.n_down) * self.n_stock\n",
    "        # the length of the data as input\n",
    "        self.n_history = max(self.n_smooth + self.history_length - 1, self.n_down * self.history_length)\n",
    "        print (\"building model....\")\n",
    "        K.clear_session()\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "        K.set_session(self.sess)\n",
    "        with self.sess.as_default():\n",
    "            self.build_model()\n",
    "        print('finished!')\n",
    "    \n",
    "    def training(self, input_data):\n",
    "        \"\"\"training DQN which consider three actions; sell, buy, hold\n",
    "              money and n_stock are considered as state variable\n",
    "        \n",
    "        Args:\n",
    "            data (list): stock price for one company\n",
    "            n_memory (int): the number of data that is used for Experience Replay\n",
    "            init_cash (float): initial available cash\n",
    "            update_target_freq (int): frequency of update for target network\n",
    "        \"\"\"\n",
    "        # since target value has large scale, we will have normalization\n",
    "        # trade_stock = stock_data[:, 0]\n",
    "        # self.scale = trade_stock[0]\n",
    "        # trade_stock = trade_stock / self.scale\n",
    "        init_op = tf.initialize_all_variables()\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        stock_data = input_data.values\n",
    "        date = input_data.index\n",
    "        T = len(stock_data)\n",
    "        print_freq = int(T / 10)\n",
    "        if print_freq == 0:\n",
    "            print_freq = 1\n",
    "        # print_freq = 1\n",
    "        print (\"training....\")\n",
    "        st = time.time()\n",
    "        result_history = []\n",
    "        portfolio = np.zeros(self.n_stock)\n",
    "        value = 0\n",
    "        K.set_session(self.sess)\n",
    "        self.sess.run(init_op)\n",
    "        memory = SequentialMemory()\n",
    "        # analyze which timing sell and buy are executed\n",
    "        # assumed that in the first n_history we do nothing\n",
    "        for t in xrange(T - 1):\n",
    "            # until having enough data, just stock data \n",
    "            if t < self.n_history:\n",
    "                action = np.zeros(self.n_stock)\n",
    "                reward = 0\n",
    "                memory.append(stock_data[t], action, reward)\n",
    "                continue\n",
    "            price = stock_data[t]\n",
    "            future_price = stock_data[t + 1]\n",
    "            # stock memory and update portfolio\n",
    "            memory.observations.append(price)\n",
    "            feature = [memory.observations[idx] for idx in xrange(memory.observations.length - self.n_history, memory.observations.length)]\n",
    "            action = np.round(self.actor_output.eval(session=self.sess,\n",
    "                                      feed_dict={self.state: [feature], K.learning_phase(): 1})[0])\n",
    "            \n",
    "            reward = np.sum((future_price - price) * action)\n",
    "            memory.rewards.append(reward)\n",
    "            memory.actions.append(action)\n",
    "            value += reward\n",
    "            # update portfolio\n",
    "            portfolio += action\n",
    "            result_history.append(reward)\n",
    "            for epoch in range(self.n_epochs):    \n",
    "                # select transition from pool\n",
    "                experiences = memory.sample(self.n_batch, self.n_history)\n",
    "                self.sess.run(self.critic_optim, \n",
    "                                feed_dict={self.state: experiences.state0,\n",
    "                                           self.state_target: experiences.state1,\n",
    "                                           self.reward: experiences.reward,\n",
    "                                           self.action: experiences.action,\n",
    "                                           self.learning_rate: self.lr,\n",
    "                                           K.learning_phase(): 1})  \n",
    "                self.sess.run(self.actor_optim,\n",
    "                                    feed_dict={self.state: experiences.state0,\n",
    "                                               self.learning_rate: self.lr,\n",
    "                                               K.learning_phase(): 1})  \n",
    "                    \n",
    "                # softupdate critic network\n",
    "                # print (\"update!\")\n",
    "                old_weights = self.critic_target.get_weights()\n",
    "                # print (\"target weights\", old_weights[0][0][0])\n",
    "                new_weights = self.critic.get_weights()\n",
    "                weights = [self.update_rate * new_w + (1 - self.update_rate) * old_w for new_w, old_w in zip(new_weights, old_weights)]\n",
    "                self.critic_target.set_weights(weights)\n",
    "                # print (\"weights\", weights[0][0][0])\n",
    "                # softupdate actor network\n",
    "                old_weights = self.actor_target.get_weights()\n",
    "                # print (\"target weights\", old_weights[0][0][0])\n",
    "                new_weights = self.actor.get_weights()\n",
    "                weights = [self.update_rate * new_w + (1 - self.update_rate) * old_w for new_w, old_w in zip(new_weights, old_weights)]\n",
    "                self.actor_target.set_weights(weights)\n",
    "                # print (\"weights\", weights[0][0][0])\n",
    "                \n",
    "                 \n",
    "            if t % print_freq == 0:\n",
    "                print (\"time:\",  date[t + 1])\n",
    "                print(\"value:\", value)\n",
    "                print(\"portfolio:\", portfolio)\n",
    "                print (\"elapsed time\", time.time() - st)    \n",
    "            \n",
    "        # save_path = saver.save(sess, \"/home/tomoaki/alpaca/notebooks/tomoaki/DQN/trained_params.ckpt\")\n",
    "        # save_path = saver.save(sess, \"/jupyter/tomoaki/DQN/trained_params.ckpt\")\n",
    "        # print(\"Model saved in file: %s\" % save_path)\n",
    "        # print (\"elapsed time: \", time.time() - st)\n",
    "        print (\"finished\")\n",
    "           \n",
    "        return np.cumsum(np.array(result_history))\n",
    "    \n",
    "    def build_model(self):\n",
    "        # just for conveninece of trainig, seprate placehoder for critic and target network\n",
    "        # critic network input  should be [raw_data, smoothed, downsampled, action]\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic_target = self.build_critic()\n",
    "        # actor network input should be [raw_data, smoothed, downsampled]\n",
    "        self.actor = self.build_actor()\n",
    "        self.actor_target = self.build_actor()\n",
    "        # transform input into the several scales and smoothing\n",
    "        self.state =  tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state')\n",
    "        self.state_target = tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state_target')\n",
    "        # reshape to convolutional input\n",
    "        state_ = tf.reshape(self.state, [-1, self.n_history, self.n_stock, 1])\n",
    "        state_target_ = tf.reshape(self.state_target, [-1, self.n_history, self.n_stock, 1])\n",
    "        raw, smoothed, down = self.transform_input(state_)\n",
    "        raw_target, smoothed_target, down_target = self.transform_input(state_target_)\n",
    "        \n",
    "        # build graph for citic training\n",
    "        self.action = tf.placeholder(tf.float32, [None, self.n_stock])\n",
    "        input_q = [raw,] +  smoothed + down + [self.action,]\n",
    "        self.Q = tf.squeeze(self.critic(input_q))#####\n",
    "        # target network\n",
    "        self.actor_target_output = self.actor_target([raw_target,] +  smoothed_target + down_target)\n",
    "        input_q_target = [raw_target,] +  smoothed_target + down_target + [self.actor_target_output,]\n",
    "        Q_target = tf.squeeze(self.critic_target(input_q_target))\n",
    "        self.reward = tf.placeholder(tf.float32, [None], name='reward')\n",
    "        target = self.reward  + self.gamma * Q_target\n",
    "        # optimization\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "        self.loss = tf.reduce_mean(tf.square(target - self.Q), name='loss')\n",
    "        self.critic_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(self.loss, var_list=self.critic.trainable_weights)\n",
    "        \n",
    "        # build graph for actor training\n",
    "        self.actor_output = self.actor([raw,] +  smoothed + down)\n",
    "        input_q_actor = [raw,] +  smoothed + down + [self.actor_output,]\n",
    "        self.Q_actor = tf.squeeze(self.critic(input_q_actor))\n",
    "        # optimization\n",
    "        self.actor_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(-self.Q_actor, var_list=self.actor.trainable_weights)\n",
    "        \n",
    "        # initialize network\n",
    "        # tf.initialize_all_variables().run(session=self.sess)\n",
    "        weights = self.critic.get_weights()\n",
    "        self.critic_target.set_weights(weights)\n",
    "        weights = self.actor.get_weights()\n",
    "        self.actor_target.set_weights(weights)\n",
    "        \n",
    "    def build_critic(self):\n",
    "        # recieve convereted tensor: raw_data, smooted_data, and downsampled_data\n",
    "        nf = self.n_feature\n",
    "        # smoothed input\n",
    "        sm_model = [Sequential() for _ in range(self.n_smooth - 1)]\n",
    "        for m in sm_model:\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # down sampled input\n",
    "        dw_model = [Sequential() for _ in range(self.n_down - 1)]\n",
    "        for m in dw_model:\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # raw input\n",
    "        state = Sequential()\n",
    "        # layer1\n",
    "        nf = self.n_feature\n",
    "        state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "        state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        state.add(PReLU())\n",
    "        merged = Merge([state,] + sm_model + dw_model, mode='concat', concat_axis=-1)\n",
    "        # layer2\n",
    "        nf = nf * 2\n",
    "        merged_state = Sequential()\n",
    "        merged_state.add(merged)\n",
    "        # model.add(SpatialDropout2D(0.5))\n",
    "        merged_state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        merged_state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        merged_state.add(PReLU())\n",
    "        merged_state.add(Flatten())\n",
    "        # layer3\n",
    "        action = Sequential()\n",
    "        action.add(Lambda(lambda x: x, input_shape=(self.n_stock,)))\n",
    "        action.add(BatchNormalization(mode=1, axis=-1))\n",
    "        merged = Merge([merged_state, action], mode='concat')\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        model.add(Dense(self.n_hidden))\n",
    "        model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # layer4\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(int(np.sqrt(self.n_hidden))))\n",
    "        # model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # output\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(1))\n",
    "        return model\n",
    "    \n",
    "    def build_actor(self):\n",
    "        # recieve convereted tensor: raw_data, smooted_data, and downsampled_data\n",
    "        nf = self.n_feature\n",
    "        # layer1\n",
    "        # smoothed input\n",
    "        sm_model = [Sequential() for _ in range(self.n_smooth - 1)]\n",
    "        for m in sm_model:\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # down sampled input\n",
    "        dw_model = [Sequential() for _ in range(self.n_down - 1)]\n",
    "        for m in dw_model:\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # raw input\n",
    "        state = Sequential()\n",
    "        state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "        state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        state.add(PReLU())\n",
    "        state.build()\n",
    "        merged = Merge([state,] + sm_model + dw_model, mode='concat')\n",
    "        # layer2\n",
    "        nf = nf * 2\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        # model.add(SpatialDropout2D(0.5))\n",
    "        model.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        model.add(BatchNormalization(mode=2 , axis=-1))\n",
    "        model.add(PReLU())\n",
    "        model.add(Flatten())\n",
    "        # layer3\n",
    "        model.add(Dense(self.n_hidden))\n",
    "        model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # layer4\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(int(np.sqrt(self.n_hidden))))\n",
    "        # model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # output\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.n_stock))\n",
    "        return model\n",
    "    \n",
    "    def transform_input(self, input):\n",
    "        # the last data is the newest information\n",
    "        raw = input[:, self.n_history - self.history_length:, :, :]\n",
    "        # smooth data\n",
    "        smoothed = []\n",
    "        for n_sm in range(2, self.n_smooth + 1):\n",
    "            smoothed.append(\n",
    "                tf.reduce_mean(tf.pack([input[:, self.n_history - st - self.history_length:self.n_history - st, :, :] for st in range(n_sm)]),0)\n",
    "            )\n",
    "        # downsample data\n",
    "        down = []\n",
    "        for n_dw in range(2, self.n_down + 1):\n",
    "            sampled_ = tf.pack([input[:, idx, :, :] for idx in range(self.n_history-n_dw*self.history_length, self.n_history, n_dw)])\n",
    "            down.append(tf.transpose(sampled_, [1, 0, 2, 3]))\n",
    "        return raw, smoothed, down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# input_data = np.zeros((505, 99))\n",
    "n_stock = len(input_data.values[0])\n",
    "\n",
    "class MultiDDPGConfig(object):\n",
    "    activation = 'relu'\n",
    "    gamma = 0.95\n",
    "    history_length = 10\n",
    "    n_stock = n_stock\n",
    "    n_smooth = 3\n",
    "    n_down = 3\n",
    "    k_w = 3\n",
    "    n_hidden = 100\n",
    "    n_batch = 32\n",
    "    n_epochs = 10\n",
    "    n_feature = 5\n",
    "    update_rate = 0.5\n",
    "    learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model....\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "tf.reset_default_graph()\n",
    "config = MultiDDPGConfig()\n",
    "\n",
    "dqn = DDPG(config)\n",
    "print (\"start!\")\n",
    "exit_idx = dqn.training(input_data)\n",
    "print (\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round([1.1, 2.452])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
