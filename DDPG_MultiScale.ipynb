{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_list = ['MMM', 'T', 'ABBV', 'ABT', 'ACN', 'AGN', 'ALL', 'GOOGL', \n",
    "              'GOOG', 'MO', 'AMZN', 'AXP', 'AIG', 'AMGN', 'AAPL', 'BAC', 'BIIB', \n",
    "              'BLK', 'BA', 'BMY', 'CVS', 'COF', 'CAT', 'CELG', 'CVX', 'CSCO', 'C', \n",
    "              'KO', 'CL', 'CMCSA', 'COP', 'COST', 'DHR', 'DOW', 'DUK', 'DD', 'EMC', \n",
    "              'EMR', 'EXC', 'XOM', 'FB', 'FDX', 'F', 'GD', 'GE', 'GM', 'GILD', 'GS', 'HAL', \n",
    "              'HD', 'HON', 'INTC', 'IBM', 'JPM', 'JNJ', 'KMI', 'LLY', 'LMT', 'LOW', 'MA', \n",
    "              'MCD', 'MDT', 'MRK', 'MET', 'MSFT', 'MDLZ', 'MON', 'MS', 'NKE', 'OXY', \n",
    "              'ORCL', 'PEP', 'PFE', 'PM', 'PG', 'QCOM', 'RTN', 'SLB', 'SPG', 'SO', 'SBUX', \n",
    "              'TGT', 'TXN', 'BK', 'PCLN', 'TWX', 'FOXA', 'FOX', 'USB', 'UNP', 'UPS', 'UTX', \n",
    "              'UNH', 'VZ', 'V', 'WMT', 'WBA', 'DIS', 'WFC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!!\n",
      "fail_name_list:  []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-81a183c414aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2015-04-01\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2016-04-01\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fixed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"time for getting data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/work/github/DQN/utils.py\u001b[0m in \u001b[0;36mget_fixed_data\u001b[0;34m(name_list, start_date, end_date, data_type)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mname_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mdate_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_datetime_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mstock_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print (\"Started!!\")\n",
    "\n",
    "st = time.time()\n",
    "start_date=\"2015-04-01\"\n",
    "end_date=\"2016-04-01\"\n",
    "input_data, input_list = utils.get_fixed_data(input_list, start_date=start_date, end_date=end_date) \n",
    "elapsed = time.time() - st\n",
    "print (\"time for getting data:\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (input_data.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(input_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG for trading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data, you are going to learn how to manage your portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# input_data = np.zeros((505, 99))\n",
    "# n_stock = len(input_data.values[0])\n",
    "n_stock = 99\n",
    "\n",
    "class MultiDDPGConfig(object):\n",
    "    activation = 'relu'\n",
    "    gamma = 0.99\n",
    "    history_length = 10\n",
    "    n_stock = n_stock\n",
    "    n_smooth = 3\n",
    "    n_down = 3\n",
    "    k_w = 3\n",
    "    n_hidden = 100\n",
    "    n_batch = 32\n",
    "    n_epochs = 100\n",
    "    n_feature = 5\n",
    "    update_rate = 1e-2\n",
    "    learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = pd.date_range('23/10/2016', periods=100, freq='D')\n",
    "value = np.random.normal(0, 1, (len(index), n_stock))\n",
    "input_data = pd.DataFrame(value, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import numpy as np\n",
    "\n",
    "Experiecne = namedtuple('Experience', 'state0, portfolio0,  action, reward, state1, portfolio1')\n",
    "\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        # self.data = [None for _ in range(maxlen)]\n",
    "        self.data = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx< 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "    \n",
    "    def append(self, v):\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item\n",
    "            self.data[1:] = self.data[:-1]\n",
    "        else:\n",
    "            # This should never happen\n",
    "            raise RuntimeError()\n",
    "        self.data.append(v)\n",
    "        \n",
    "class SequentialMemory(object):\n",
    "    def __init__(self, limit=1000):\n",
    "        self.limit = limit\n",
    "        \n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "        self.portfolios = RingBuffer(limit)\n",
    "        \n",
    "    def sample(self, batch_size, window_length):\n",
    "        # draw random indexes such that is bigger than window_length to enough length data\n",
    "        batch_idx = np.random.random_integers(window_length, self.nb_entries - 1, size=batch_size - 1)\n",
    "        # take the newest data\n",
    "        batch_idx = np.concatenate((batch_idx, [self.nb_entries - 1]))\n",
    "        assert len(batch_idx) == batch_size\n",
    "        \n",
    "        # create experiences\n",
    "        state0 = np.array([[self.observations[i] for i in range(idx - window_length,idx)] for idx in batch_idx])\n",
    "        portfolio0 = np.array([self.portfolios[idx - 1] for idx in batch_idx])\n",
    "        action = np.array([self.actions[idx - 1] for idx in batch_idx])\n",
    "        reward = np.array([self.rewards[idx - 1] for idx in batch_idx])\n",
    "        state1 = np.array([[self.observations[i] for i in range(idx - window_length + 1,idx + 1)] for idx in batch_idx])\n",
    "        portfolio1 = np.array([self.portfolios[idx] for idx in batch_idx])\n",
    "        return Experiecne(state0, portfolio0, action, reward, state1, portfolio1)\n",
    "    \n",
    "    def append(self, observation, portfolio, action, reward):\n",
    "        self.observations.append(observation)\n",
    "        self.portfolios.append(portfolio)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return  len(self.observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D \n",
    "from keras.layers.core import Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.engine.topology import Merge\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# from memory import SequentialMemory\n",
    "\n",
    "class DDPG(object):\n",
    "    \"\"\"Deep Deterministic Poilicy Gradient\n",
    "    \n",
    "    Basend on DPG and Multiscale CNN, seek out \n",
    "    optimal strategy for stock trading.\n",
    "    \n",
    "    Available function\n",
    "    - build_model: build network basedon tensorflow and keras\n",
    "    - train: given DateFrame stock data, train network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"initialized approximate value function\n",
    "        \n",
    "        config should have the following attributes\n",
    "        \n",
    "        Args:\n",
    "            gamma(float): the decay rate for value at RL\n",
    "            history_length(int): input_length for each scale at CNN\n",
    "            n_feature(int): the number of type of input \n",
    "                (e.g. the number of company to use at stock trading)\n",
    "            trade_stock_idx(int): trading stock index\n",
    "            gam (float): discount factor\n",
    "            n_history(int): the nubmer of history that will be used as input\n",
    "            n_smooth, n_down(int): the number of smoothed and down sampling input at CNN\n",
    "            k_w(int): the size of filter at CNN\n",
    "            n_hidden(int): the size of fully connected layer\n",
    "            n_batch(int): the size of mini batch\n",
    "            n_epochs(int): the training epoch for each time\n",
    "            update_rate (0, 1): parameter for soft update\n",
    "            learning_rate(float): learning rate for SGD\n",
    "        \"\"\"\n",
    "        self.gamma = config.gamma\n",
    "        self.history_length = config.history_length\n",
    "        self.n_stock = config.n_stock\n",
    "        self.n_feature = config.n_feature\n",
    "        self.n_smooth = config.n_smooth\n",
    "        self.n_down = config.n_down\n",
    "        self.k_w = config.k_w\n",
    "        self.n_hidden = config.n_hidden\n",
    "        self.n_batch = config.n_batch\n",
    "        self.n_epochs = config.n_epochs\n",
    "        self.update_rate = config.update_rate\n",
    "        self.lr = config.learning_rate\n",
    "        # the actual dimention of input\n",
    "        self.n_input = (1 + self.n_smooth + self.n_down) * self.n_stock\n",
    "        # the length of the data as input\n",
    "        self.n_history = max(self.n_smooth + self.history_length - 1, self.n_down * self.history_length)\n",
    "        print (\"building model....\")\n",
    "        # have compatibility with new tensorflow\n",
    "        tf.python.control_flow_ops = tf\n",
    "        # avoid creating _LEARNING_PHASE outside the network\n",
    "        K.clear_session()\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "        K.set_session(self.sess)\n",
    "        with self.sess.as_default():\n",
    "            self.build_model()\n",
    "        print('finished building model!')\n",
    "    \n",
    "    def train(self, input_data, noise_scale=3.0):\n",
    "        \"\"\"training DDPG, where action is confined to integer space\n",
    "        \n",
    "        Args:\n",
    "            data (DataFrame): stock price for self.n_feature companies\n",
    "        \"\"\"\n",
    "        stock_data = input_data.values\n",
    "        date = input_data.index\n",
    "        T = len(stock_data)\n",
    "        \n",
    "        # frequency for output\n",
    "        print_freq = int(T / 100)\n",
    "        if print_freq == 0:\n",
    "            print_freq = 1\n",
    "            \n",
    "        print (\"training....\")\n",
    "        st = time.time()\n",
    "        # the number of poassed stocks for each company\n",
    "        portfolio = np.zeros(self.n_stock)\n",
    "        # result for return value\n",
    "        value = 0\n",
    "        values = []\n",
    "        date_label = []\n",
    "        values.append(value)\n",
    "        date_label.append(date[0])\n",
    "        # repeat memory for DQN\n",
    "        memory = SequentialMemory()\n",
    "        for t in range(T - 1):\n",
    "            portfolio_off = np.zeros(self.n_stock)\n",
    "            portfolio_on = np.zeros(self.n_stock)\n",
    "            # until having enough data, just do nothing\n",
    "            if t <= self.n_history + 50:\n",
    "                action = np.zeros(self.n_stock)\n",
    "                reward = 0\n",
    "                memory.append(stock_data[t], portfolio_off, action, reward)\n",
    "                continue\n",
    "            price = stock_data[t]\n",
    "            future_price = stock_data[t + 1]\n",
    "            # to stabilize batch normalization, use other samples for prediction\n",
    "            experiences = memory.sample(self.n_batch, self.n_history)\n",
    "            memory.observations.append(price)\n",
    "            # off policy action and update portfolio\n",
    "            pred_state = experiences.state0\n",
    "            pred_portfolio_off = np.concatenate((experiences.portfolio0[:-1], [portfolio_off]))\n",
    "            actor_value_off = self.actor_output.eval(session=self.sess,\n",
    "                                      feed_dict={self.state: pred_state,\n",
    "                                                          self.portfolio: pred_portfolio_off,\n",
    "                                                          K.learning_phase(): 0})[-1]\n",
    "            action_off = np.round(actor_value_off + np.random.normal(0, noise_scale))\n",
    "            portfolio_off += action_off\n",
    "            reward_off = np.sum((future_price - price) * portfolio_off)\n",
    "            memory.portfolios.append(portfolio_off)\n",
    "            memory.rewards.append(reward_off)\n",
    "            memory.actions.append(action_off)\n",
    "            # on policy action and update portfolio\n",
    "            pred_portfolio_on = np.concatenate((experiences.portfolio0[:-1], [portfolio_on]))\n",
    "            actor_value_on = self.actor_output.eval(session=self.sess,\n",
    "                                      feed_dict={self.state: pred_state,\n",
    "                                                          self.portfolio: pred_portfolio_on,\n",
    "                                                          K.learning_phase(): 0})[-1]\n",
    "            action_on = np.round(actor_value_on)\n",
    "            portfolio_on += action_on\n",
    "            reward_on = np.sum((future_price - price) * portfolio_on)\n",
    "            value += reward_on\n",
    "            values.append(value)\n",
    "            date_label.append(date[t+1])\n",
    "            \n",
    "            # update network\n",
    "            for epoch in range(self.n_epochs):    \n",
    "                # select transition from pool\n",
    "                experiences = memory.sample(self.n_batch, self.n_history)\n",
    "                self.sess.run(self.critic_optim, \n",
    "                              feed_dict={self.state: experiences.state0,\n",
    "                                         self.state_target: experiences.state1,\n",
    "                                         self.reward: experiences.reward,\n",
    "                                         self.action: experiences.action,\n",
    "                                         self.portfolio: experiences.portfolio0,\n",
    "                                         self.portfolio_target: experiences.portfolio1,\n",
    "                                         self.learning_rate: self.lr,\n",
    "                                         K.learning_phase(): 1})  \n",
    "                self.sess.run(self.actor_optim,\n",
    "                                       feed_dict={self.state: experiences.state0,\n",
    "                                                            self.portfolio: experiences.portfolio0,\n",
    "                                                            self.learning_rate: self.lr,\n",
    "                                                            K.learning_phase(): 1})  \n",
    "                    \n",
    "                # softupdate for critic network\n",
    "                old_weights = self.critic_target.get_weights()\n",
    "                new_weights = self.critic.get_weights()\n",
    "                weights = [self.update_rate * new_w + (1 - self.update_rate) * old_w for new_w, old_w in zip(new_weights, old_weights)]\n",
    "                self.critic_target.set_weights(weights)\n",
    "                \n",
    "                # softupdate for actor network\n",
    "                old_weights = self.actor_target.get_weights()\n",
    "                new_weights = self.actor.get_weights()\n",
    "                weights = [self.update_rate * new_w + (1 - self.update_rate) * old_w for new_w, old_w in zip(new_weights, old_weights)]\n",
    "                self.actor_target.set_weights(weights)             \n",
    "                 \n",
    "            if t % print_freq == 0:\n",
    "                print (\"time:\",  date[t + 1])\n",
    "                print(\"value:\", value)\n",
    "                print(\"portfolio:\", portfolio_on)\n",
    "                print (\"elapsed time\", time.time() - st)    \n",
    "\n",
    "        print (\"finished training\")\n",
    "           \n",
    "        return pd.DataFrame(values, index=pd.DatetimeIndex(date_label))\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build all of the network and optimizations\n",
    "        \n",
    "        just for conveninece of trainig, seprate placehoder for train and target network\n",
    "        critic network input: [raw_data, smoothed, downsampled, action]\n",
    "        actor network input: [raw_data, smoothed, downsampled]\n",
    "        \"\"\"\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic_target = self.build_critic()\n",
    "        # actor network input should be [raw_data, smoothed, downsampled]\n",
    "        self.actor = self.build_actor()\n",
    "        self.actor_target = self.build_actor()\n",
    "        # transform input into the several scales and smoothing\n",
    "        self.state =  tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state')\n",
    "        self.state_target = tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state_target')\n",
    "        # reshape to convolutional input\n",
    "        state_ = tf.reshape(self.state, [-1, self.n_history, self.n_stock, 1])\n",
    "        state_target_ = tf.reshape(self.state_target, [-1, self.n_history, self.n_stock, 1])\n",
    "        raw, smoothed, down = self.transform_input(state_)\n",
    "        raw_target, smoothed_target, down_target = self.transform_input(state_target_)\n",
    "        \n",
    "        # build graph for citic training\n",
    "        self.action = tf.placeholder(tf.float32, [None, self.n_stock])\n",
    "        self.portfolio = tf.placeholder(tf.float32, [None, self.n_stock])\n",
    "        self.portfolio_target = tf.placeholder(tf.float32, [None, self.n_stock])\n",
    "        input_q = [raw,] +  smoothed + down + [self.action,]  + [self.portfolio, ]\n",
    "        self.Q = tf.squeeze(self.critic(input_q))\n",
    "        # target network\n",
    "        self.actor_target_output = self.actor_target([raw_target,] +  smoothed_target + down_target + [self.portfolio_target, ])\n",
    "        input_q_target = [raw_target,] +  smoothed_target + down_target + [self.actor_target_output,] + [self.portfolio_target, ]\n",
    "        Q_target = tf.squeeze(self.critic_target(input_q_target))\n",
    "        self.reward = tf.placeholder(tf.float32, [None], name='reward')\n",
    "        target = self.reward  + self.gamma * Q_target\n",
    "        # optimization\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "        self.loss = tf.reduce_mean(tf.square(target - self.Q), name='loss')\n",
    "        self.critic_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(self.loss, var_list=self.critic.trainable_weights)\n",
    "        \n",
    "        # build graph for actor training\n",
    "        self.actor_output = self.actor([raw,] +  smoothed + down + [self.portfolio, ])\n",
    "        input_q_actor = [raw,] +  smoothed + down + [self.actor_output,] + [self.portfolio,]\n",
    "        self.Q_actor = tf.squeeze(self.critic(input_q_actor))\n",
    "        # optimization\n",
    "        self.actor_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(-self.Q_actor, var_list=self.actor.trainable_weights)\n",
    "        \n",
    "        # initialize network\n",
    "        tf.initialize_all_variables().run(session=self.sess)\n",
    "        weights = self.critic.get_weights()\n",
    "        self.critic_target.set_weights(weights)\n",
    "        weights = self.actor.get_weights()\n",
    "        self.actor_target.set_weights(weights)\n",
    "        \n",
    "    def build_critic(self):\n",
    "        \"\"\"Build critic network\n",
    "        \n",
    "        recieve convereted tensor: raw_data, smooted_data, and downsampled_data\n",
    "        \"\"\"\n",
    "        nf = self.n_feature\n",
    "        # layer1\n",
    "        # smoothed input\n",
    "        sm_model = [Sequential() for _ in range(self.n_smooth - 1)]\n",
    "        for m in sm_model:\n",
    "            m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(SpatialDropout2D(0.2))\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # down sampled input\n",
    "        dw_model = [Sequential() for _ in range(self.n_down - 1)]\n",
    "        for m in dw_model:\n",
    "            m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(SpatialDropout2D(0.2))\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # raw input\n",
    "        state = Sequential()\n",
    "        nf = self.n_feature\n",
    "        state.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n",
    "        state.add(SpatialDropout2D(0.2))\n",
    "        state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        state.add(PReLU())\n",
    "        merged = Merge([state,] + sm_model + dw_model, mode='concat', concat_axis=-1)\n",
    "        # layer2\n",
    "        nf = nf * 2\n",
    "        merged_state = Sequential()\n",
    "        merged_state.add(merged)\n",
    "        merged_state.add(SpatialDropout2D(0.2))\n",
    "        merged_state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        merged_state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        merged_state.add(PReLU())\n",
    "        merged_state.add(Flatten())\n",
    "        # layer3\n",
    "        action = Sequential()\n",
    "        action.add(Lambda(lambda x: x, input_shape=(self.n_stock,)))\n",
    "        action.add(BatchNormalization(mode=1, axis=-1))\n",
    "        portfolio = Sequential()\n",
    "        portfolio.add(Lambda(lambda x: x, input_shape=(self.n_stock,)))\n",
    "        portfolio.add(BatchNormalization(mode=1, axis=-1))\n",
    "        merged = Merge([merged_state, action, portfolio], mode='concat')\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        model.add(Dense(self.n_hidden))\n",
    "        model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # layer4\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(int(np.sqrt(self.n_hidden))))\n",
    "        model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # output\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1))\n",
    "        return model\n",
    "    \n",
    "    def build_actor(self):\n",
    "        \"\"\"Build actor network\n",
    "        \n",
    "        recieve convereted tensor: raw_data, smooted_data, and downsampled_data\n",
    "        \"\"\"\n",
    "        nf = self.n_feature\n",
    "        # layer1\n",
    "        # smoothed input\n",
    "        sm_model = [Sequential() for _ in range(self.n_smooth - 1)]\n",
    "        for m in sm_model:\n",
    "            m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            # m.add(SpatialDropout2D(0.2))\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # down sampled input\n",
    "        dw_model = [Sequential() for _ in range(self.n_down - 1)]\n",
    "        for m in dw_model:\n",
    "            m.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            # m.add(SpatialDropout2D(0.2))\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # raw input     \n",
    "        state = Sequential()\n",
    "        state.add(Lambda(lambda x: x,  input_shape=(self.history_length, self.n_stock, 1)))\n",
    "        # state.add(SpatialDropout2D(0.2))\n",
    "        state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        state.add(PReLU())\n",
    "        merged = Merge([state,] + sm_model + dw_model, mode='concat')\n",
    "        # layer2\n",
    "        nf = nf * 2\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        model.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        model.add(BatchNormalization(mode=2 , axis=-1))\n",
    "        model.add(PReLU())\n",
    "        model.add(Flatten())\n",
    "        portfolio = Sequential()\n",
    "        portfolio.add(Lambda(lambda x: x, input_shape=(self.n_stock,)))\n",
    "        portfolio.add(BatchNormalization(mode=1, axis=-1))\n",
    "        merged = Merge([model, portfolio],  mode='concat')\n",
    "        # layer3\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        model.add(Dense(self.n_hidden))\n",
    "        model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # output\n",
    "        model.add(Dense(self.n_stock))\n",
    "        return model\n",
    "    \n",
    "    def transform_input(self, input):\n",
    "        # the last data is the newest information\n",
    "        raw = input[:, self.n_history - self.history_length:, :, :]\n",
    "        # smooth data\n",
    "        smoothed = []\n",
    "        for n_sm in range(2, self.n_smooth + 1):\n",
    "            smoothed.append(\n",
    "                tf.reduce_mean(tf.pack([input[:, self.n_history - st - self.history_length:self.n_history - st, :, :] for st in range(n_sm)]),0)\n",
    "            )\n",
    "        # downsample data\n",
    "        down = []\n",
    "        for n_dw in range(2, self.n_down + 1):\n",
    "            sampled_ = tf.pack([input[:, idx, :, :] for idx in range(self.n_history-n_dw*self.history_length, self.n_history, n_dw)])\n",
    "            down.append(tf.transpose(sampled_, [1, 0, 2, 3]))\n",
    "        return raw, smoothed, down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model....\n",
      "finished building model!\n",
      "start!\n",
      "training....\n",
      "time: 2017-01-13 00:00:00\n",
      "value: -6.12678617841\n",
      "portfolio: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "elapsed time 98.24562215805054\n"
     ]
    }
   ],
   "source": [
    "# from model import DDPG\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config = MultiDDPGConfig()\n",
    "\n",
    "dqn = DDPG(config)\n",
    "print (\"start!\")\n",
    "values = dqn.train(input_data)\n",
    "print (\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DatetimeIndex' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-48cc7b9e9bfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DatetimeIndex' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "print(values.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff5e03ebd50>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAD+CAYAAAD26kgrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzuFiZmbZOVzMzCy7/w8JYu55rNTUagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff5994b89d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_label = input_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-04-01 00:00:00')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_list = list(date_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2015-04-01', '2015-04-02', '2015-04-06', '2015-04-07',\n",
       "               '2015-04-08', '2015-04-09', '2015-04-10', '2015-04-13',\n",
       "               '2015-04-14', '2015-04-15',\n",
       "               ...\n",
       "               '2016-03-18', '2016-03-21', '2016-03-22', '2016-03-23',\n",
       "               '2016-03-24', '2016-03-28', '2016-03-29', '2016-03-30',\n",
       "               '2016-03-31', '2016-04-01'],\n",
       "              dtype='datetime64[ns]', length=253, freq=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DatetimeIndex(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
