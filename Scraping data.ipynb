{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, sys, time, string\n",
    "from urllib import request\n",
    "from urllib import error\n",
    "import logging\n",
    "\n",
    "class FetchAllSybols(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # URL\n",
    "        # default m (market) - IN, t (type) - S (stock)\n",
    "        self.sym_start_url = \"https://in.finance.yahoo.com/lookup/stocks?t=S&m=IN\"\n",
    "        self.sym_page_url = '&b=0'#page\n",
    "        self.sym_alphanum_search_url = '&s=a' #search alphabet a\n",
    "        self.sym_full_url = ''\n",
    "        self.alphabet_str_to_search = string.ascii_lowercase # full alphabet\n",
    "        self.sym_info = {}\n",
    "        self.header = \"SymbolId,Full Name, Type, Exchange, URL\\n\" \n",
    "        \n",
    "    def set_alphabet_in_url(self, alphabet):\n",
    "        \"\"\" \n",
    "        Set the alphabet portion of the url by passing the alphabet.\n",
    "        :param alphbet (str): can be alphabet.\n",
    "        \"\"\"\n",
    "        self.sym_alphanum_search_url = '&s=' + str(alphabet)\n",
    "\n",
    "    def set_pagenumber_in_url(self, pageno):\n",
    "        \"\"\" \n",
    "        Set the page portion of the url by passing the pageno.\n",
    "        :param pageno (str): page number.\n",
    "        \"\"\"\n",
    "        self.sym_page_url = '&b=' + str(pageno)\n",
    "\n",
    "    \n",
    "    def gen_next_url(self):\n",
    "        \"\"\" \n",
    "        Creates the full url necessary for sym scan by joining the search parameter and page no.\n",
    "        \"\"\"\n",
    "        self.sym_full_url =  self.sym_start_url + self.sym_alphanum_search_url + self.sym_page_url    \n",
    "    \n",
    "    def get_data_from_next_page(self):\n",
    "        self.gen_next_url()\n",
    "        print (\"Fetching data from URL\", self.sym_full_url)\n",
    "        req = request.Request(self.sym_full_url, headers={ 'User-Agent': 'Mozilla/5.0' })\n",
    "        html = None\n",
    "        counter = 0\n",
    "        while counter < 10:\n",
    "            try:\n",
    "                html = request.urlopen(req)\n",
    "            except error.HTTPError:\n",
    "                logging.error(error.read())\n",
    "                logging.info(\"Will try 10 times with 2 seconds sleep\")\n",
    "                time.sleep(2) \n",
    "                counter += 1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        soup = BeautifulSoup(html, \"html\")\n",
    "        return soup\n",
    "    \n",
    "    def get_all_valid_symbols(self):\n",
    "        \"\"\" \n",
    "        Scan all the symbol for one page. The parsing are split into odd and even rows.\n",
    "        \"\"\"\n",
    "        soup = self.get_data_from_next_page()\n",
    "        table = soup.find_all(\"div\", class_=\"yui-content\")\n",
    "        table_rows = table[0].find_all('tr')\n",
    "        for table_row in table_rows:\n",
    "            if table_row.find('td'):\n",
    "                if table_row.contents[2].text != 'NaN':\n",
    "                    self.sym_info[table_row.contents[0].text]=[table_row.contents[1].text,\n",
    "                                                               table_row.contents[3].text,\n",
    "                                                               table_row.contents[4].text,\n",
    "                                                               table_row.a[\"href\"]]\n",
    "                    \n",
    "    def get_total_page_to_scan_for_alphabet(self, alphabet):\n",
    "        \"\"\" \n",
    "        Get the total search results based on each search to determine the number of page to scan.\n",
    "        :param alphabet (int): The total number of page to scan\n",
    "        \"\"\"\n",
    "        self.sym_start_url = \"https://in.finance.yahoo.com/lookup/stocks?t=S&m=IN&r=\"\n",
    "        self.sym_page_url = '&b=0'#page\n",
    "        self.sym_alphanum_search_url = '&s='+alphabet\n",
    "        \n",
    "        soup = self.get_data_from_next_page()\n",
    "        total_search_str = (str(soup.find_all(\"div\", id=\"pagination\")))\n",
    "        \n",
    "        #Get the number of page\n",
    "        total_search_qty = re.search('of ([1-9]*\\,*[0-9]*).*',total_search_str).group(1)\n",
    "        total_search_qty = int(total_search_qty.replace(',','', total_search_qty.count(',')))\n",
    "        final_search_page_count = total_search_qty/20 #20 seach per page.\n",
    " \n",
    "        return final_search_page_count\n",
    "    \n",
    "    \n",
    "    def get_total_sym_for_each_search(self, alphabet):\n",
    "        \"\"\" \n",
    "        Scan all the page indicate by the search item.\n",
    "        The first time search or the first page will get the total number of search.\n",
    "        Dividing it by 20 results per page will give the number of page to search.\n",
    "        :param alphabet(str)\n",
    "        \"\"\"\n",
    "        # Get the first page info first\n",
    "        self.set_pagenumber_in_url(0)\n",
    "        total_page_to_scan =  self.get_total_page_to_scan_for_alphabet(alphabet)\n",
    "        logging.info('Total number of pages to scan: [%d]'% total_page_to_scan)\n",
    "\n",
    "        # Scan the rest of the page.\n",
    "        # may need to get time to rest\n",
    "        for page_no in range(0,total_page_to_scan+1,1):\n",
    "            self.set_pagenumber_in_url(page_no*20)\n",
    "            self.gen_next_url()\n",
    "            logging.info('Scanning page number: [%d] url: [%s]  ' % (page_no, self.sym_full_url))          \n",
    "            self.get_all_valid_symbols()\n",
    "    \n",
    "    def serach_for_each_alphabet(self):\n",
    "        \"\"\" \n",
    "        Sweep through all the alphabets to get the full list of shares.\n",
    "        \"\"\"\n",
    "        for alphabet in self.alphabet_str_to_search:\n",
    "            logging.info('Searching for : [%s]' % alphabet)\n",
    "            self.set_alphabet_in_url(alphabet)\n",
    "            self.get_total_sym_for_each_search(alphabet)\n",
    "    \n",
    "    def dump_into_file(self):\n",
    "        '''\n",
    "        Store all symbols into a csv file.\n",
    "        '''\n",
    "        f = open('Symbol_Info.csv', 'w')\n",
    "        f.write(self.header)\n",
    "        \n",
    "        for key in sorted(self.sym_info):\n",
    "            values = self.sym_info[key]\n",
    "            sym = key+','\n",
    "            for value in values:\n",
    "                sym += str(value) + ','\n",
    "            sym += '\\n'\n",
    "            f.write(sym)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from URL https://in.finance.yahoo.com/lookup/stocks?t=S&m=IN&s=a&b=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "f = FetchAllSybols()\n",
    "f.get_all_valid_symbols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
