{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/admin/work\")\n",
    "import my_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_list = ['MMM', 'T', 'ABBV', 'ABT', 'ACN', 'AGN', 'ALL', 'GOOGL', \n",
    "              'GOOG', 'MO', 'AMZN', 'AXP', 'AIG', 'AMGN', 'AAPL', 'BAC', 'BIIB', \n",
    "              'BLK', 'BA', 'BMY', 'CVS', 'COF', 'CAT', 'CELG', 'CVX', 'CSCO', 'C', \n",
    "              'KO', 'CL', 'CMCSA', 'COP', 'COST', 'DHR', 'DOW', 'DUK', 'DD', 'EMC', \n",
    "              'EMR', 'EXC', 'XOM', 'FB', 'FDX', 'F', 'GD', 'GE', 'GM', 'GILD', 'GS', 'HAL', \n",
    "              'HD', 'HON', 'INTC', 'IBM', 'JPM', 'JNJ', 'KMI', 'LLY', 'LMT', 'LOW', 'MA', \n",
    "              'MCD', 'MDT', 'MRK', 'MET', 'MSFT', 'MDLZ', 'MON', 'MS', 'NKE', 'OXY', \n",
    "              'ORCL', 'PEP', 'PFE', 'PM', 'PG', 'QCOM', 'RTN', 'SLB', 'SPG', 'SO', 'SBUX', \n",
    "              'TGT', 'TXN', 'BK', 'PCLN', 'TWX', 'FOXA', 'FOX', 'USB', 'UNP', 'UPS', 'UTX', \n",
    "              'UNH', 'VZ', 'V', 'WMT', 'WBA', 'DIS', 'WFC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!!\n",
      "fail_name_list:  []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-645f8b006ae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2015-04-01\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2016-04-01\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fixed_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"time for getting data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/admin/work/my_utils.py\u001b[0m in \u001b[0;36mget_fixed_data\u001b[0;34m(name_list, start_date, end_date, data_type)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_time_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print (\"Started!!\")\n",
    "\n",
    "st = time.time()\n",
    "start_date=\"2015-04-01\"\n",
    "end_date=\"2016-04-01\"\n",
    "input_data, date, input_list = my_utils.get_fixed_data(input_list, start_date=start_date, end_date=end_date) \n",
    "elapsed = time.time() - st\n",
    "print (\"time for getting data:\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "symbols = symbol_list['Symbol'].values\n",
    "print (symbols)\n",
    "print (len(symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_list = list(symbols[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!!\n",
      "('fail_name_list: ', ['ABEOW', 'ACIA', 'OILD', 'OILU', 'AITP', 'ADXSW', 'AGLE', 'UAVS', 'AGFSW'])\n",
      "time for getting data: 324.233609915\n"
     ]
    }
   ],
   "source": [
    "print (\"Started!!\")\n",
    "\n",
    "st = time.time()\n",
    "start_date=\"2015-04-01\"\n",
    "end_date=\"2016-04-01\"\n",
    "input_data, date, input_list = my_utils.get_fixed_data(input_list, start_date=start_date, end_date=end_date) \n",
    "elapsed = time.time() - st\n",
    "print (\"time for getting data:\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 98)\n"
     ]
    }
   ],
   "source": [
    "print (input_data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "n = len(input_data[0])\n",
    "print (n)\n",
    "print (len(input_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "new_input = []\n",
    "for idx, data in enumerate(input_data):\n",
    "    if len(data) == n:\n",
    "        new_input.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = np.array(new_input).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98, 253)\n"
     ]
    }
   ],
   "source": [
    "print (input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_label = my_utils.get_datetime_list(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "stock_data = np.zeros((505, 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG for trading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given data, you are going to learn how to manage your portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# memory for replay\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Experiecne = namedtuple('Experience', 'state0, action, reward, state1')\n",
    "\n",
    "class RingBuffer(object):\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.start = 0\n",
    "        self.length = 0\n",
    "        self.data = [None for _ in range(maxlen)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx< 0 or idx >= self.length:\n",
    "            raise KeyError()\n",
    "        return self.data[(self.start + idx) % self.maxlen]\n",
    "    \n",
    "    def append(self, v):\n",
    "        if self.length < self.maxlen:\n",
    "            # We have space, simply increase the length\n",
    "            self.length += 1\n",
    "        elif self.length == self.maxlen:\n",
    "            # No space, \"remove\" the first item\n",
    "            self.data[1:] = self.data[:-1]\n",
    "        else:\n",
    "            # This should never happen\n",
    "            raise RuntimeError()\n",
    "        self.data.append(v)\n",
    "        \n",
    "class SequentialMemory(object):\n",
    "    def __init__(self, limit):\n",
    "        self.limit = limit\n",
    "        \n",
    "        self.actions = RingBuffer(limit)\n",
    "        self.rewards = RingBuffer(limit)\n",
    "        self.observations = RingBuffer(limit)\n",
    "        \n",
    "    def sample(self, batch_size, window_length):\n",
    "        # draw random indexes such that is bigger than window_length to enough length data\n",
    "        batch_idxs = np.random.random_integers(window_length, self.nb_entries - 1, size=batch_size)\n",
    "        assert len(batch_idxs) == batch_size\n",
    "        \n",
    "        # create experiences \n",
    "        state0 = [self.observations[idx - window_length:idx] for idx in batch_idx]\n",
    "        action = [self.actions[idx - 1] for idx in batch_idx]\n",
    "        reward = [self.rewards[idx - 1] for idx in batch_idx]\n",
    "        state1 = [self.observations[idx - window_length + 1:idx+1] for idx in batch_idx]\n",
    "        return Experiecne(state0, action, reward, state1)\n",
    "    \n",
    "    def append(self, observation, action, reward):\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return  len(self.observations)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D \n",
    "from keras.layers.core import Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.engine.topology import Merge\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D \n",
    "from keras.layers.core import Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.engine.topology import Merge\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import SpatialDropout2D\n",
    "from keras.layers import Dropout\n",
    "from keras import backend as K\n",
    "# from keras.layers.core import K\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"initialized approximate value function\n",
    "        config should have the following attributes\n",
    "        Args:\n",
    "            trade_stock_idx(int): trading stock index\n",
    "            gam (float): discount factor\n",
    "            n_history(int): nubmer of history that will be used as input\n",
    "        \"\"\"\n",
    "        self.activation = config.activation\n",
    "        self.gamma = config.gamma\n",
    "        self.history_length = config.history_length\n",
    "        self.n_stock = config.n_stock\n",
    "        self.n_smooth = config.n_smooth\n",
    "        self.n_feature = config.n_feature\n",
    "        self.n_down = config.n_down\n",
    "        self.k_w = config.k_w\n",
    "        self.n_hidden = config.n_hidden\n",
    "        self.n_batch = config.n_batch\n",
    "        self.n_epoch = config.n_epoch\n",
    "        self.n_memory = config.n_memory\n",
    "        self.learning_rate = config.learning_rate\n",
    "        self.n_history = max(self.n_smooth + self.history_length - 1, self.n_down * self.history_length)\n",
    "        K.clear_session()\n",
    "        # automatically choose device \n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "        print (\"building model....\")\n",
    "        with self.sess.as_default():\n",
    "            with tf.device('/gpu:0'):\n",
    "                K.set_session(self.sess)\n",
    "                self.build_model()\n",
    "        print('finished!')\n",
    "    \n",
    "    def training(self, stock_data):\n",
    "        \"\"\"training DQN which consider three actions; sell, buy, hold\n",
    "              money and n_stock are considered as state variable\n",
    "        \n",
    "        Args:\n",
    "            data (list): stock price for one company\n",
    "            n_memory (int): the number of data that is used for Experience Replay\n",
    "            init_cash (float): initial available cash\n",
    "            update_target_freq (int): frequency of update for target network\n",
    "        \"\"\"\n",
    "        # since target value has large scale, we will have normalization\n",
    "        # trade_stock = stock_data[:, 0]\n",
    "        # self.scale = trade_stock[0]\n",
    "        # trade_stock = trade_stock / self.scale\n",
    "        init_op = tf.initialize_all_variables()\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        T = len(stock_data)\n",
    "        print_freq = int(T / 10)\n",
    "        if print_freq == 0:\n",
    "            print_freq = 1\n",
    "        # print_freq = 1\n",
    "        print (\"training....\")\n",
    "        st = time.time()\n",
    "        result_history = []\n",
    "        portfolio = np.zeros(self.n_stock)\n",
    "        K.set_session(self.sess)\n",
    "        self.sess.run(init_op)\n",
    "        memory = SequentialMemory(self.n_memory)\n",
    "        lr = self.learning_rate\n",
    "        # analyze which timing sell and buy are executed\n",
    "        # assumed that in the first n_history we do nothing\n",
    "        for t in range(T - 1):\n",
    "            # until having enough data, just stock data \n",
    "            if t < self.n_history:\n",
    "                memory.append(stock_data[t], None, None)\n",
    "                continue\n",
    "            price = stock_data[t]\n",
    "            future_price = stock_data[t + 1]\n",
    "            # stock memory and update portfolio\n",
    "            memory.observations.append(price)\n",
    "            feature = memory.observations[-self.n_history:]\n",
    "            action = self.action.eval(session=sess,\n",
    "                                      feed_dict={self.state: [feature]})[0]\n",
    "            reward = (future_price - price) * action\n",
    "            memory.rewards.append(reward)\n",
    "            memory.actions.append(action)\n",
    "            # update portfolio\n",
    "            poftfolio += action\n",
    "            result_history.append(reward)\n",
    "            for epoch in range(self.n_epochs):    \n",
    "                # select transition from pool\n",
    "                experiences = memory.sample(self.n_batch, self.n_history)\n",
    "                    \n",
    "                sess.run(self.critic_optim, \n",
    "                                feed_dict={self.state: experiences.state0,\n",
    "                                                     self.state_target: experiences.state1,\n",
    "                                                     self.reward: experiences.reward,\n",
    "                                                     self.acrtion: experiences.action,\n",
    "                                                    self._learning_rate: lr,\n",
    "                                                     K.learning_phase(): 0})  \n",
    "                sess.run(self.actor_optim, \n",
    "                                feed_dict={self.state: experiences.state0,\n",
    "                                           self._learning_rate: lr,\n",
    "                                            K.learning_phase(): 0})  \n",
    "                    \n",
    "                # softupdate critic network\n",
    "                print (\"update!\")\n",
    "                old_weights = self.critic_target.get_weights()\n",
    "                print (\"target weights\", old_weights[0][0][0])\n",
    "                new_weights = self.critic.get_weights()\n",
    "                weights = [self.update_rate * new_w + (1 - self.udpate_rate) * old_w for new_w, old_w in zip(new_weights, old_weights)]\n",
    "                self.critic_target.set_weights(weights)\n",
    "                print (\"weights\", weights[0][0][0])\n",
    "                # softupdate actor network\n",
    "                old_weights = self.actor_target.get_weights()\n",
    "                print (\"target weights\", old_weights[0][0][0])\n",
    "                new_weights = self.actor.get_weights()\n",
    "                weights = [self.update_rate * new_w + (1 - self.udpate_rate) * old_w for new_w, old_w in zip(new_weights, old_weights)]\n",
    "                self.actor_target.set_weights(weights)\n",
    "                print (\"weights\", weights[0][0][0])\n",
    "                \n",
    "            if date is None:\n",
    "                tm = t\n",
    "            else:\n",
    "                tm = date[t]\n",
    "                \n",
    "            if t % print_freq == 0:\n",
    "                print (\"time:\",  (tm))\n",
    "                print (\"elapsed time\", time.time() - st)    \n",
    "            \n",
    "        # save_path = saver.save(sess, \"/home/tomoaki/alpaca/notebooks/tomoaki/DQN/trained_params.ckpt\")\n",
    "        # save_path = saver.save(sess, \"/jupyter/tomoaki/DQN/trained_params.ckpt\")\n",
    "        # print(\"Model saved in file: %s\" % save_path)\n",
    "        # print (\"elapsed time: \", time.time() - st)\n",
    "        print (\"finished\")\n",
    "           \n",
    "        return np.cumsum(np.array(result_history))\n",
    "    \n",
    "    def build_model(self):\n",
    "        K.set_learning_phase(0)\n",
    "        # just for conveninece of trainig, seprate placehoder for critic and target network\n",
    "        # critic network input  should be [raw_data, smoothed, downsampled, action]\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic_target = self.build_critic()\n",
    "        # actor network input should be [raw_data, smoothed, downsampled]\n",
    "        self.actor = self.build_actor()\n",
    "        self.actor_target = self.build_actor()\n",
    "        # transform input into the several scales and smoothing\n",
    "        self.state =  tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state')\n",
    "        self.state_target = tf.placeholder(tf.float32, [None, self.n_history, self.n_stock], name='state_target')\n",
    "        # reshape to convolutional input\n",
    "        state_ = tf.reshape(self.state, [-1, self.n_history, self.n_stock, 1])\n",
    "        state_target_ = tf.reshape(self.state_target, [-1, self.n_history, self.n_stock, 1])\n",
    "        converted_state = self.transform_input(state_)\n",
    "        converted_state_target = self.transform_input(state_target_)\n",
    "        \n",
    "        # build graph for citic training\n",
    "        self.action = tf.placeholder(tf.float32, [None, self.n_stock])\n",
    "        self.Q = tf.squeeze(self.critic(converted_state + [self.action,]))\n",
    "        # target network\n",
    "        self.actor_target_output = self.actor(converted_state_target)\n",
    "        Q_target = tf.squeeze(self.critic_target(converted_state_target + [self.actor_target_output, ]))\n",
    "        self.reward = tf.placeholder(tf.float32, [None], name='reward')\n",
    "        target = self.reward  + self.gamma * Q_target\n",
    "        # optimization\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "        self.loss = tf.reduce_mean(tf.square(target - self.Q), name='loss')\n",
    "        self.critic_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(self.loss, var_list=self.critic.trainable_weights)\n",
    "        \n",
    "        # build graph for actor training\n",
    "        self.actor_output = self.actor(converted_state)\n",
    "        self.Q_actor = tf.squeeze(self.critic(converted_state + [self.actor_output,]))\n",
    "        # optimization\n",
    "        self.actor_optim = tf.train.AdamOptimizer(self.learning_rate) \\\n",
    "            .minimize(-self.Q_actor, var_list=self.actor.trainable_weights)\n",
    "        \n",
    "        # initialize network\n",
    "        tf.initialize_all_variables().run(session=self.sess)\n",
    "        weights = self.critic.get_weights()\n",
    "        self.critic_target.set_weights(weights)\n",
    "        weights = self.actor.get_weights()\n",
    "        self.actor_target.set_weights(weights)\n",
    "        \n",
    "    def build_critic(self):\n",
    "        K.set_learning_phase(0)\n",
    "        # recieve convereted tensor: raw_data, smooted_data, and downsampled_data\n",
    "        nf = self.n_feature\n",
    "        # smoothed input\n",
    "        sm_model = [Sequential() for _ in range(self.n_smooth - 1)]\n",
    "        for m in sm_model:\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # down sampled input\n",
    "        dw_model = [Sequential() for _ in range(self.n_down - 1)]\n",
    "        for m in dw_model:\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # raw input\n",
    "        state = Sequential()\n",
    "        # layer1\n",
    "        nf = self.n_feature\n",
    "        state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "        state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        state.add(PReLU())\n",
    "        merged = Merge([state,] + sm_model + dw_model, mode='concat', concat_axis=-1)\n",
    "        # layer2\n",
    "        nf = nf * 2\n",
    "        merged_state = Sequential()\n",
    "        merged_state.add(merged)\n",
    "        # model.add(SpatialDropout2D(0.5))\n",
    "        merged_state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        merged_state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        merged_state.add(PReLU())\n",
    "        merged_state.add(Flatten())\n",
    "        # layer3\n",
    "        action = Sequential()\n",
    "        action.add(Lambda(lambda x: x, input_shape=(self.n_stock,)))\n",
    "        action.add(BatchNormalization(mode=1, axis=-1))\n",
    "        merged = Merge([merged_state, action], mode='concat')\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        model.add(Dense(self.n_hidden))\n",
    "        model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # layer4\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(int(np.sqrt(self.n_hidden))))\n",
    "        # model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # output\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(1))\n",
    "        return model\n",
    "    \n",
    "    def build_actor(self):\n",
    "        K.set_learning_phase(0)\n",
    "        # recieve convereted tensor: raw_data, smooted_data, and downsampled_data\n",
    "        nf = self.n_feature\n",
    "        # layer1\n",
    "        # smoothed input\n",
    "        sm_model = [Sequential() for _ in range(self.n_smooth - 1)]\n",
    "        for m in sm_model:\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # down sampled input\n",
    "        dw_model = [Sequential() for _ in range(self.n_down - 1)]\n",
    "        for m in dw_model:\n",
    "            m.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "            m.add(BatchNormalization(mode=2, axis=-1))\n",
    "            m.add(PReLU())\n",
    "        # raw input\n",
    "        state = Sequential()\n",
    "        state.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same', input_shape=(self.history_length, self.n_stock, 1)))\n",
    "        state.add(BatchNormalization(mode=2, axis=-1))\n",
    "        state.add(PReLU())\n",
    "        state.build()\n",
    "        merged = Merge([state,] + sm_model + dw_model, mode='concat')\n",
    "        # layer2\n",
    "        nf = nf * 2\n",
    "        model = Sequential()\n",
    "        model.add(merged)\n",
    "        # model.add(SpatialDropout2D(0.5))\n",
    "        model.add(Convolution2D(nb_filter=nf, nb_row=self.k_w, nb_col=1, border_mode='same'))\n",
    "        model.add(BatchNormalization(mode=2 , axis=-1))\n",
    "        model.add(PReLU())\n",
    "        model.add(Flatten())\n",
    "        # layer3\n",
    "        model.add(Dense(self.n_hidden))\n",
    "        model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # layer4\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(int(np.sqrt(self.n_hidden))))\n",
    "        # model.add(BatchNormalization(mode=1, axis=-1))\n",
    "        model.add(PReLU())\n",
    "        # output\n",
    "        # model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.n_stock))\n",
    "        return model\n",
    "    \n",
    "    def transform_input(self, input):\n",
    "        \"\"\"transfrom data into n_smoothed smoothed_data and n_down scales downsampled data\n",
    "        Args:\n",
    "            input: placeholder that have the shape, (n_batch, n_history, n_stock, 1)\n",
    "        Return:\n",
    "            list of tensors\n",
    "        \"\"\"\n",
    "        # the last data is the newest information\n",
    "        raw = input[:, self.n_history - self.history_length:, :, :]\n",
    "        # smooth data\n",
    "        smoothed = []\n",
    "        for n_sm in range(2, self.n_smooth + 1):\n",
    "            smoothed.append(\n",
    "                tf.reduce_mean(tf.pack([input[:, self.n_history - st - self.history_length:self.n_history - st, :, :] for st in range(n_sm)]),0)\n",
    "            )\n",
    "        # downsample data\n",
    "        down = []\n",
    "        for n_dw in range(2, self.n_down + 1):\n",
    "            sampled_ = tf.pack([input[:, idx, :, :] for idx in range(self.n_history-n_dw*(self.history_length - 1) - 1, self.n_history, n_dw)])\n",
    "            down.append(tf.transpose(sampled_, [1, 0, 2, 3]))\n",
    "        return [raw,] +  smoothed + down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_stock = len(input_data[0])\n",
    "n_stock = 100\n",
    "\n",
    "class MultiDDPGConfig(object):\n",
    "    activation = 'relu'\n",
    "    gamma = 0.95\n",
    "    history_length = 10\n",
    "    n_memory = 1000\n",
    "    n_stock = n_stock\n",
    "    n_smooth = 3\n",
    "    n_down = 3\n",
    "    k_w = 3\n",
    "    n_hidden = 100\n",
    "    n_batch = 32\n",
    "    n_epoch = 10\n",
    "    n_feature = 10\n",
    "    learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda/lib/python3.5/site-packages/keras/engine/topology.py:1660: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_8_model\" was not an Input tensor, it was generated by layer lambda_1.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_1:0\n",
      "  str(x.name))\n",
      "/Users/admin/anaconda/lib/python3.5/site-packages/keras/engine/topology.py:1660: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_7_model\" was not an Input tensor, it was generated by layer lambda_1.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_1:0\n",
      "  str(x.name))\n",
      "/Users/admin/anaconda/lib/python3.5/site-packages/keras/engine/topology.py:1660: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_16_model\" was not an Input tensor, it was generated by layer lambda_2.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_2:0\n",
      "  str(x.name))\n",
      "/Users/admin/anaconda/lib/python3.5/site-packages/keras/engine/topology.py:1660: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"sequential_15_model\" was not an Input tensor, it was generated by layer lambda_2.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: lambda_input_2:0\n",
      "  str(x.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n",
      "start!\n",
      "training....\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unorderable types: slice() < int()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d70d4b9bfc16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"start!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mexit_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"finished!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-880a98571967>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, stock_data)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# stock memory and update portfolio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_history\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             action = self.action.eval(session=sess,\n\u001b[1;32m    105\u001b[0m                                       feed_dict={self.state: [feature]})[0]\n",
      "\u001b[0;32m<ipython-input-47-3f59ba384444>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unorderable types: slice() < int()"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "tf.reset_default_graph()\n",
    "config = MultiDDPGConfig()\n",
    "\n",
    "dqn = DDPG(config)\n",
    "print (\"start!\")\n",
    "exit_idx = dqn.training(stock_data)\n",
    "print (\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
